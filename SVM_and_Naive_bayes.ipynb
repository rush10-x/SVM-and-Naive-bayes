{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **THEORITICAL QUESTIONS**"
      ],
      "metadata": {
        "id": "5eoXlN9dBZzL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q1. What is a Support Vector Machine (SVM)?\n",
        "- A Support Vector Machine (SVM) is a supervised machine learning algorithm used for classification and regression tasks. It works by finding the optimal hyperplane that best separates the data points of different classes with the maximum margin between them.\n",
        "\n",
        "\n",
        "Q2. What is the difference between Hard Margin and Soft Margin SVM?\n",
        "\n",
        "- The key difference between Hard Margin and Soft Margin SVM lies in how they handle data that is not perfectly separable. A Hard Margin SVM assumes that the data is completely linearly separable, meaning it tries to find a hyperplane that classifies all training examples correctly without allowing any misclassifications. This approach works well only when there is a clear margin between the classes and no overlap, but it is very sensitive to noise and outliers.\n",
        "\n",
        "- On the other hand, a Soft Margin SVM introduces flexibility by allowing some data points to be on the wrong side of the margin or even misclassified. This is achieved by introducing slack variables and a regularization parameter C, which balances the trade-off between maximizing the margin and minimizing classification error. The Soft Margin approach is more robust and better suited for real-world datasets where perfect separation is rare due to noise or overlapping classes.\n",
        "\n",
        "Q3. What is the mathematical intuition behind SVM?\n",
        "- SVM tries to:\n",
        "\n",
        "- Maximize the margin between the support vectors (closest points of each class).\n",
        "\n",
        "Formulate the problem as an optimization task:\n",
        "\n",
        "min\n",
        "⁡\n",
        "𝑤\n",
        ",\n",
        "𝑏\n",
        "1\n",
        "2\n",
        "∥\n",
        "𝑤\n",
        "∥\n",
        "2\n",
        "subject to\n",
        "𝑦\n",
        "𝑖\n",
        "(\n",
        "𝑤\n",
        "𝑇\n",
        "𝑥\n",
        "𝑖\n",
        "+\n",
        "𝑏\n",
        ")\n",
        "≥\n",
        "1\n",
        "w,b\n",
        "min\n",
        "​\n",
        "  \n",
        "2\n",
        "1\n",
        "​\n",
        " ∥w∥\n",
        "2\n",
        " subject to y\n",
        "i\n",
        "​\n",
        " (w\n",
        "T\n",
        " x\n",
        "i\n",
        "​\n",
        " +b)≥1\n",
        "\n",
        "- In Soft Margin, slack variables\n",
        "𝜉\n",
        "𝑖\n",
        "ξ\n",
        "i\n",
        "​\n",
        "  are introduced to allow margin violations:\n",
        "\n",
        "min\n",
        "⁡\n",
        "1\n",
        "2\n",
        "∥\n",
        "𝑤\n",
        "∥\n",
        "2\n",
        "+\n",
        "𝐶\n",
        "∑\n",
        "𝜉\n",
        "𝑖\n",
        "min\n",
        "2\n",
        "1\n",
        "​\n",
        " ∥w∥\n",
        "2\n",
        " +C∑ξ\n",
        "i\n",
        "​\n",
        "\n",
        "\n",
        "Q4. What is the role of Lagrange Multipliers in SVM?\n",
        "- Lagrange multipliers are used to:\n",
        "\n",
        "- Convert the constrained optimization problem into an unconstrained dual problem.\n",
        "\n",
        "- Allow solving the problem more efficiently, especially with kernels.\n",
        "\n",
        "- In dual form, only support vectors have non-zero Lagrange multipliers\n",
        "𝛼\n",
        "𝑖\n",
        "α\n",
        "i\n",
        "​\n",
        " , which determine the final model:\n",
        "\n",
        "𝑤\n",
        "=\n",
        "∑\n",
        "𝛼\n",
        "𝑖\n",
        "𝑦\n",
        "𝑖\n",
        "𝑥\n",
        "𝑖\n",
        "w=∑α\n",
        "i\n",
        "​\n",
        " y\n",
        "i\n",
        "​\n",
        " x\n",
        "i\n",
        "​\n",
        "\n",
        "\n",
        "\n",
        "Q5. What are Support Vectors in SVM?\n",
        "- Support Vectors are:\n",
        "\n",
        "The data points that lie closest to the decision boundary (hyperplane).\n",
        "\n",
        "They are critical because they define the position and orientation of the hyperplane.\n",
        "\n",
        "Only these points directly affect the model; removing them would change the margin.\n",
        "\n"
      ],
      "metadata": {
        "id": "TTgFrGmKBgJG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q6. What is a Support Vector Classifier (SVC)?\n",
        "- A Support Vector Classifier (SVC) is a type of Support Vector Machine (SVM) used for classification tasks. It finds the optimal hyperplane that best separates data points of different classes by maximizing the margin between the closest points (support vectors) of each class.\n",
        "\n",
        "\n",
        "Q7. What is a Support Vector Regressor (SVR)?\n",
        "- A Support Vector Regressor (SVR) is the regression version of SVM. Instead of classifying data, it tries to fit a function within a margin of tolerance (ε) from the actual data points. The goal is to find a line (or curve) that predicts continuous values with minimal error, ignoring errors within the epsilon margin.\n",
        "\n",
        "\n",
        "Q8. What is the Kernel Trick in SVM?\n",
        "- The Kernel Trick is a method used in SVM to transform data into a higher-dimensional space without explicitly computing the transformation. It enables SVM to solve non-linear problems by applying kernel functions (like RBF or Polynomial) to compute the dot product in the transformed space, allowing linear separation in that space.\n",
        "\n",
        "Q9. Compare Linear Kernel, Polynomial Kernel, and RBF Kernel:\n",
        "- The Linear Kernel, Polynomial Kernel, and RBF (Radial Basis Function) Kernel are commonly used kernel functions in SVM, each suited for different types of data. The Linear Kernel is the simplest and is used when the data is linearly separable—meaning it can be divided by a straight line (or hyperplane). It is efficient and works well in high-dimensional spaces, such as text classification. The Polynomial Kernel introduces more complexity by mapping the data into a higher-dimensional space using polynomial functions. This is useful when the relationship between the features and the target is curved or non-linear. The RBF Kernel, also known as the Gaussian Kernel, is the most widely used and can handle complex and highly non-linear relationships by considering the distance between data points. It transforms the data into an infinite-dimensional space, allowing SVM to draw flexible boundaries between classes. In summary, the choice of kernel depends on the nature of the data: use linear when the data is simple, polynomial for moderate non-linearity, and RBF for highly non-linear and complex patterns.\n",
        "\n",
        "Q10. What is the effect of the C parameter in SVM?\n",
        "- The C parameter controls the trade-off between a smooth decision boundary and classifying training points correctly:\n",
        "\n",
        "A small C makes the margin wider, allowing some misclassifications (more generalization).\n",
        "\n",
        "A large C tries to classify all training points correctly, which may lead to overfitting."
      ],
      "metadata": {
        "id": "lbyr2xdgD7JG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q11. What is the role of the Gamma parameter in RBF Kernel SVM?\n",
        "- Gamma (γ) defines how far the influence of a single training example reaches in an RBF (Radial Basis Function) kernel. A low gamma means a large similarity radius, so points far away from each other are considered similar, leading to smoother decision boundaries. A high gamma makes the similarity radius smaller, meaning the model focuses tightly around each support vector, which can lead to overfitting. Essentially, gamma controls the curvature of the decision boundary.\n",
        "\n",
        "Q12. What is the Naïve Bayes classifier, and why is it called \"Naïve\"?\n",
        "- Naïve Bayes is a probabilistic classifier based on Bayes’ Theorem. It assumes that all features are independent given the class label. It is called \"naïve\" because this independence assumption is rarely true in real data — yet the model still works well in many practical scenarios, especially in text classification and spam filtering.\n",
        "\n",
        "Q13. What is Bayes’ Theorem?\n",
        "- Bayes’ Theorem describes the probability of an event based on prior knowledge of conditions that might be related to the event. It is mathematically stated as:\n",
        "P(A∣B)=\n",
        "P(B)\n",
        "P(B∣A)⋅P(A)\n",
        "​\n",
        "\n",
        "Where:\n",
        "\n",
        "𝑃\n",
        "(\n",
        "𝐴\n",
        "∣\n",
        "𝐵\n",
        ")\n",
        "P(A∣B) is the posterior probability: probability of A given B.\n",
        "\n",
        "𝑃\n",
        "(\n",
        "𝐵\n",
        "∣\n",
        "𝐴\n",
        ")\n",
        "P(B∣A) is the likelihood: probability of B given A.\n",
        "\n",
        "𝑃\n",
        "(\n",
        "𝐴\n",
        ")\n",
        "P(A) is the prior probability of A.\n",
        "\n",
        "𝑃\n",
        "(\n",
        "𝐵\n",
        ")\n",
        "P(B) is the evidence or probability of B\n",
        "\n",
        "Q14. Explain the differences between Gaussian Naïve Bayes, Multinomial Naïve Bayes, and Bernoulli Naïve Bayes:\n",
        "\n",
        "- Gaussian Naïve Bayes: Assumes that the features follow a normal (Gaussian) distribution. Suitable for continuous data, such as heights, weights, or sensor readings.\n",
        "\n",
        "- Multinomial Naïve Bayes: Used for discrete/count data, especially in text classification where features are word counts or frequencies.\n",
        "\n",
        "- Bernoulli Naïve Bayes: Designed for binary/boolean features (0s and 1s), such as whether a word is present or not in a document.\n",
        "\n",
        "Q15. When should you use Gaussian Naïve Bayes over other variants?\n",
        "\n",
        "- You should use Gaussian Naïve Bayes when your features are continuous and approximately normally distributed. It is ideal for datasets where attributes are real-valued and do not represent counts or binary indicators — for example, in medical data, sensor data, or financial indicators."
      ],
      "metadata": {
        "id": "GS1WWERkEX65"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q16. What are the key assumptions made by Naïve Bayes?\n",
        "\n",
        "\n",
        "The main assumption of Naïve Bayes is conditional independence:\n",
        "\n",
        "- It assumes that all features are independent of each other given the class label.\n",
        "\n",
        "- That is, the presence (or absence) of a feature does not depend on the presence (or absence) of any other feature, given the class.\n",
        "\n",
        "This assumption is rarely true in real-world data, but the model still performs well in many applications due to its simplicity and efficiency.\n",
        "\n",
        "Q17. What are the advantages and disadvantages of Naïve Bayes?\n",
        "- Advantages:\n",
        "\n",
        "Simple and fast to train and predict.\n",
        "\n",
        "Works well with high-dimensional data, such as text classification.\n",
        "\n",
        "Performs well even with small datasets.\n",
        "\n",
        "Not sensitive to irrelevant features.\n",
        "\n",
        "Requires less training data.\n",
        "\n",
        "- Disadvantages:\n",
        "\n",
        "Strong assumption of feature independence, which is often unrealistic.\n",
        "\n",
        "Poor estimation of probabilities (not calibrated).\n",
        "\n",
        "Doesn't perform well when features are highly correlated.\n",
        "\n",
        "Struggles with zero-frequency problem (handled by Laplace smoothing).\n",
        "\n",
        "\n",
        "\n",
        "Q18. Why is Naïve Bayes a good choice for text classification?\n",
        "\n",
        "- Text data is typically high-dimensional and sparse—Naïve Bayes handles both efficiently.\n",
        "\n",
        "Words (features) in documents are often treated as conditionally independent given the class, which fits Naïve Bayes assumptions reasonably well.\n",
        "\n",
        "It is fast to train and predict, even on large text datasets.\n",
        "\n",
        "Works well with bag-of-words and TF-IDF features.\n",
        "\n",
        "Despite its simplicity, it often achieves competitive accuracy in real-world NLP tasks.\n",
        "\n",
        "\n",
        "\n",
        "Q19. Compare SVM and Naïve Bayes for classification tasks:\n",
        "- Support Vector Machines (SVM) and Naïve Bayes are both popular classification algorithms, but they differ significantly in their approach. Naïve Bayes is a probabilistic model that relies on the assumption of conditional independence among features, meaning it assumes that each feature contributes independently to the outcome. In contrast, SVM is a deterministic model that aims to find the optimal hyperplane that best separates different classes in the feature space, without making any assumptions about feature independence. Naïve Bayes is very fast to train and performs well on high-dimensional, sparse datasets like text classification, where independence assumptions somewhat hold. However, it may suffer when features are highly correlated or when probabilities are important. On the other hand, SVM generally provides higher accuracy in complex classification problems and can model non-linear decision boundaries using kernel tricks, though it is computationally more intensive. Unlike Naïve Bayes, SVM does not naturally provide probability estimates unless additional calibration is applied. Overall, Naïve Bayes is preferred for speed and simplicity, especially in text tasks, while SVM is often chosen for higher accuracy and robustness in complex, real-world classification problems.\n",
        "\n",
        "Q20. How does Laplace Smoothing help in Naïve Bayes?\n",
        "- Laplace Smoothing (also called add-one smoothing) helps Naïve Bayes by addressing the zero-frequency problem.\n",
        "\n",
        "If a word (feature) in test data was never seen in training data for a particular class, its probability becomes zero, which can zero out the entire product of probabilities.\n",
        "\n",
        "Laplace smoothing adds a small constant (usually 1) to all feature counts to avoid zero probabilities.\n",
        "\n",
        "Formula (for categorical data):\n",
        "\n",
        "𝑃\n",
        "(\n",
        "𝑥\n",
        "𝑖\n",
        "∣\n",
        "𝑦\n",
        ")\n",
        "=\n",
        "count\n",
        "(\n",
        "𝑥\n",
        "𝑖\n",
        ",\n",
        "𝑦\n",
        ")\n",
        "+\n",
        "1\n",
        "/ count\n",
        "(\n",
        "𝑦\n",
        ")\n",
        "+\n",
        "𝑉\n",
        "\n",
        "\n",
        "Where V is the total number of possible feature values (e.g., vocabulary size)."
      ],
      "metadata": {
        "id": "4oxDFBGYFj8-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **PRACTICAL QUESTIONS**"
      ],
      "metadata": {
        "id": "oXnQMNvAGeMa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Q21. Write a Python program to train an SVM Classifier on the Iris dataset and evaluate accuracy\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "iris = datasets.load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Split dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train SVM classifier\n",
        "model = SVC(kernel='linear')\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict and evaluate\n",
        "y_pred = model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Iris SVM Classifier Accuracy: {accuracy:.2f}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "jJ3S21LgGhdp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q22. Write a Python program to train two SVM classifiers with Linear and RBF kernels on the Wine dataset, then compare their accuracies\n",
        "\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "wine = load_wine()\n",
        "X, y = wine.data, wine.target\n",
        "\n",
        "# Split dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Linear kernel\n",
        "linear_svm = SVC(kernel='linear')\n",
        "linear_svm.fit(X_train, y_train)\n",
        "linear_accuracy = accuracy_score(y_test, linear_svm.predict(X_test))\n",
        "\n",
        "# RBF kernel\n",
        "rbf_svm = SVC(kernel='rbf')\n",
        "rbf_svm.fit(X_train, y_train)\n",
        "rbf_accuracy = accuracy_score(y_test, rbf_svm.predict(X_test))\n",
        "\n",
        "print(f\"Wine Dataset - Linear Kernel Accuracy: {linear_accuracy:.2f}\")\n",
        "print(f\"Wine Dataset - RBF Kernel Accuracy: {rbf_accuracy:.2f}\")\n"
      ],
      "metadata": {
        "id": "5msldullHFT8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q23.  Write a Python program to train an SVM Regressor (SVR) on a housing dataset and evaluate it using Mean Squared Error (MSE)\n",
        "\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Load dataset\n",
        "housing = fetch_california_housing()\n",
        "X, y = housing.data, housing.target\n",
        "\n",
        "# Split dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train SVR model\n",
        "model = SVR(kernel='rbf')\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict and evaluate\n",
        "y_pred = model.predict(X_test)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(f\"Housing SVR Model Mean Squared Error: {mse:.2f}\")\n"
      ],
      "metadata": {
        "id": "JJlhzPGqHLME"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q24. Write a Python program to train an SVM Classifier with a Polynomial Kernel and visualize the decision boundary\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Generate synthetic 2D dataset\n",
        "X, y = make_classification(n_samples=200, n_features=2, n_redundant=0,\n",
        "                           n_informative=2, n_clusters_per_class=1, random_state=42)\n",
        "\n",
        "# Feature scaling\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Train SVM with Polynomial Kernel\n",
        "clf = SVC(kernel='poly', degree=3, C=1.0)\n",
        "clf.fit(X_scaled, y)\n",
        "\n",
        "# Plot decision boundary\n",
        "def plot_decision_boundary(clf, X, y):\n",
        "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
        "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
        "    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 500),\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "DfgPOTUcHce9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q25. Write a Python program to train a Gaussian Naïve Bayes classifier on the Breast Cancer dataset and evaluate accuracy\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load Breast Cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split dataset into training and testing sets (70% train, 30% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Initialize Gaussian Naive Bayes classifier\n",
        "gnb = GaussianNB()\n",
        "\n",
        "# Train the classifier\n",
        "gnb.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test data\n",
        "y_pred = gnb.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy of Gaussian Naive Bayes classifier: {accuracy:.4f}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "2zIctIhdHlwy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q26. Write a Python program to train a Multinomial Naïve Bayes classifier for text classification using the 20 Newsgroups dataset.\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "\n",
        "# Load the 20 Newsgroups dataset (subset for speed)\n",
        "newsgroups = fetch_20newsgroups(subset='all', shuffle=True, random_state=42)\n",
        "\n",
        "X = newsgroups.data\n",
        "y = newsgroups.target\n",
        "\n",
        "# Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Create a pipeline to vectorize, transform with TF-IDF, and train MultinomialNB\n",
        "model = Pipeline([\n",
        "    ('vect', CountVectorizer(stop_words='english')),\n",
        "    ('tfidf', TfidfTransformer()),\n",
        "    ('clf', MultinomialNB())\n",
        "])\n",
        "\n",
        "# Train the classifier\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test data\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate accuracy and classification report\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy:.4f}\\n\")\n",
        "print(\"Classification Report:\\n\", classification_report(y_test, y_pred, target_names=newsgroups.target_names))\n"
      ],
      "metadata": {
        "id": "e8hjGW6OHsXc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q27. Write a Python program to train an SVM Classifier with different C values and compare the decision boundaries visually\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import datasets\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "# Generate a simple 2D dataset for visualization\n",
        "X, y = datasets.make_blobs(n_samples=100, centers=2, random_state=6, cluster_std=1.5)\n",
        "\n",
        "# Define function to plot decision boundary\n",
        "def plot_decision_boundary(clf, X, y, ax, title):\n",
        "    ax.scatter(X[:, 0], X[:, 1], c=y, cmap='coolwarm', s=30, edgecolors='k')\n",
        "    ax.set_title(title)\n",
        "    ax.set_xlim(X[:, 0].min()-1, X[:, 0].max()+1)\n",
        "    ax.set_ylim(X[:, 1].min()-1, X[:, 1].max()+1)\n",
        "\n",
        "    # Create grid to evaluate model\n",
        "    xx, yy = np.meshgrid(np.linspace(X[:, 0].min()-1, X[:, 0].max()+1, 500),\n",
        "                         np.linspace(X[:, 1].min()-1, X[:, 1].max()+1, 500))\n",
        "    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "    Z = Z.reshape(xx.shape)\n",
        "\n",
        "    ax.contourf(xx, yy, Z, alpha=0.2, cmap='coolwarm')\n",
        "\n",
        "# Different C values to try\n",
        "C_values = [0.1, 1, 10, 100]\n",
        "\n",
        "fig, axes = plt.subplots(1, len(C_values), figsize=(16, 4))\n",
        "\n",
        "for i, C in enumerate(C_values):\n",
        "    clf = SVC(kernel='linear', C=C)\n",
        "    clf.fit(X, y)\n",
        "    plot_decision_boundary(clf, X, y, axes[i], f'C = {C}')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "mgcokE5FIAFp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q28. Write a Python program to train a Bernoulli Naïve Bayes classifier for binary classification on a dataset with binary features\n",
        "from sklearn.naive_bayes import BernoulliNB\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Example binary feature dataset (toy data)\n",
        "X = np.array([[1,0,1,0,1],\n",
        "              [0,1,0,1,0],\n",
        "              [1,1,1,0,0],\n",
        "              [0,0,0,1,1],\n",
        "              [1,0,0,1,0],\n",
        "              [0,1,1,0,1],\n",
        "              [1,1,0,0,0],\n",
        "              [0,0,1,1,1]])\n",
        "y = np.array([1, 0, 1, 0, 1, 0, 1, 0])  # Binary target\n",
        "\n",
        "# Split dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
        "\n",
        "# Train Bernoulli Naive Bayes\n",
        "bnb = BernoulliNB()\n",
        "bnb.fit(X_train, y_train)\n",
        "\n",
        "# Predict & evaluate\n",
        "y_pred = bnb.predict(X_test)\n",
        "print(f\"Accuracy: {accuracy_score(y_test, y_pred):.2f}\")\n"
      ],
      "metadata": {
        "id": "wzwN9__aNQtR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q29. Write a Python program to apply feature scaling before training an SVM model and compare results with unscaled data\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# For simplicity, do binary classification (class 0 and 1)\n",
        "X = X[y != 2]\n",
        "y = y[y != 2]\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
        "\n",
        "# Train SVM without scaling\n",
        "svm_unscaled = SVC(kernel='rbf', C=1)\n",
        "svm_unscaled.fit(X_train, y_train)\n",
        "y_pred_unscaled = svm_unscaled.predict(X_test)\n",
        "acc_unscaled = accuracy_score(y_test, y_pred_unscaled)\n",
        "\n",
        "# Scale features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Train SVM with scaled data\n",
        "svm_scaled = SVC(kernel='rbf', C=1)\n",
        "svm_scaled.fit(X_train_scaled, y_train)\n",
        "y_pred_scaled = svm_scaled.predict(X_test_scaled)\n",
        "acc_scaled = accuracy_score(y_test, y_pred_scaled)\n",
        "\n",
        "print(f\"Accuracy without scaling: {acc_unscaled:.2f}\")\n",
        "print(f\"Accuracy with scaling: {acc_scaled:.2f}\")\n"
      ],
      "metadata": {
        "id": "oR2T4N8bNWos"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q30. Write a Python program to train a Gaussian Naïve Bayes model and compare the predictions before and after Laplace Smoothing\n",
        "import numpy as np\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load data\n",
        "data = load_iris()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train GaussianNB without smoothing\n",
        "gnb = GaussianNB()\n",
        "gnb.fit(X_train, y_train)\n",
        "y_pred_no_smooth = gnb.predict(X_test)\n",
        "acc_no_smooth = accuracy_score(y_test, y_pred_no_smooth)\n",
        "\n",
        "# Implement smoothing by adding small value to variance (sigma^2)\n",
        "class GaussianNBLaplace(GaussianNB):\n",
        "    def __init__(self, var_smoothing=1e-9):\n",
        "        super().__init__()\n",
        "        self.var_smoothing = var_smoothing\n",
        "\n",
        "    def _update_variance(self, X):\n",
        "        # Override variance calculation to add smoothing\n",
        "        var = np.var(X, axis=0) + self.var_smoothing\n",
        "        return var\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        self.classes_ = np.unique(y)\n",
        "        self.class_prior_ = np.array([np.mean(y == c) for c in self.classes_])\n",
        "        self.theta_ = np.array([X[y == c].mean(axis=0) for c in self.classes_])\n",
        "        self.sigma_ = np.array([self._update_variance(X[y == c]) for c in self.classes_])\n",
        "        return self\n",
        "\n",
        "laplace_gnb = GaussianNBLaplace(var_smoothing=1e-2)  # increased smoothing\n",
        "laplace_gnb.fit(X_train, y_train)\n",
        "y_pred_smooth = laplace_gnb.predict(X_test)\n",
        "acc_smooth = accuracy_score(y_test, y_pred_smooth)\n",
        "\n",
        "print(f\"Accuracy without smoothing: {acc_no_smooth:.4f}\")\n",
        "print(f\"Accuracy with smoothing (variance smoothing): {acc_smooth:.4f}\")\n"
      ],
      "metadata": {
        "id": "tHWEqyw1Nb5y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q31. Write a Python program to train an SVM Classifier and use GridSearchCV to tune the hyperparameters (C, gamma, kernel)\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Load data\n",
        "data = load_iris()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Define model\n",
        "svc = SVC()\n",
        "\n",
        "# Hyperparameter grid\n",
        "param_grid = {\n",
        "    'C': [0.1, 1, 10, 100],\n",
        "    'gamma': ['scale', 'auto', 0.01, 0.1, 1],\n",
        "    'kernel': ['linear', 'rbf', 'poly']\n",
        "}\n",
        "\n",
        "# GridSearchCV\n",
        "grid_search = GridSearchCV(svc, param_grid, cv=5, n_jobs=-1, verbose=1)\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "print(\"Best parameters found:\", grid_search.best_params_)\n",
        "\n",
        "# Predict and evaluate on test set\n",
        "best_svc = grid_search.best_estimator_\n",
        "y_pred = best_svc.predict(X_test)\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n"
      ],
      "metadata": {
        "id": "-kOWjuA8Nhzk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q32. Write a Python program to train an SVM Classifier on an imbalanced dataset and apply class weighting and check it improve accuracy\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "import numpy as np\n",
        "\n",
        "# Create imbalanced dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=20, n_classes=2,\n",
        "                           weights=[0.9, 0.1], flip_y=0, random_state=42)\n",
        "\n",
        "# Split dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train SVM without class weighting\n",
        "svc_no_weight = SVC()\n",
        "svc_no_weight.fit(X_train, y_train)\n",
        "y_pred_no_weight = svc_no_weight.predict(X_test)\n",
        "acc_no_weight = accuracy_score(y_test, y_pred_no_weight)\n",
        "\n",
        "# Train SVM with class weighting balanced\n",
        "svc_weighted = SVC(class_weight='balanced')\n",
        "svc_weighted.fit(X_train, y_train)\n",
        "y_pred_weighted = svc_weighted.predict(X_test)\n",
        "acc_weighted = accuracy_score(y_test, y_pred_weighted)\n",
        "\n",
        "print(f\"Accuracy without class weighting: {acc_no_weight:.4f}\")\n",
        "print(f\"Accuracy with class weighting: {acc_weighted:.4f}\")\n",
        "\n",
        "print(\"\\nClassification report without weighting:\\n\", classification_report(y_test, y_pred_no_weight))\n",
        "print(\"\\nClassification report with weighting:\\n\", classification_report(y_test, y_pred_weighted))\n"
      ],
      "metadata": {
        "id": "mN4PVqETNnkE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q33. Write a Python program to implement a Naïve Bayes classifier for spam detection using email data\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "\n",
        "# Example dataset (you can replace this with your own email dataset)\n",
        "data = {\n",
        "    'text': [\n",
        "        \"Free money now!!!\",\n",
        "        \"Hi, how are you?\",\n",
        "        \"Win a free ticket\",\n",
        "        \"Are we meeting today?\",\n",
        "        \"You won a prize\",\n",
        "        \"Let's catch up tomorrow\",\n",
        "        \"Cheap meds available\",\n",
        "        \"Can you call me back?\",\n",
        "        \"Urgent! Claim your reward\",\n",
        "        \"See you at the party\"\n",
        "    ],\n",
        "    'label': ['spam', 'ham', 'spam', 'ham', 'spam', 'ham', 'spam', 'ham', 'spam', 'ham']\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(df['text'], df['label'], test_size=0.3, random_state=42)\n",
        "\n",
        "# Convert text data to numeric vectors\n",
        "vectorizer = CountVectorizer()\n",
        "X_train_counts = vectorizer.fit_transform(X_train)\n",
        "X_test_counts = vectorizer.transform(X_test)\n",
        "\n",
        "# Train Naive Bayes classifier\n",
        "nb_classifier = MultinomialNB()\n",
        "nb_classifier.fit(X_train_counts, y_train)\n",
        "\n",
        "# Predict and evaluate\n",
        "y_pred = nb_classifier.predict(X_test_counts)\n",
        "\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n"
      ],
      "metadata": {
        "id": "hHLIUYr-NvJ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q34.  Write a Python program to train an SVM Classifier and a Naïve Bayes Classifier on the same dataset and compare their accuracy\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "# Using the same dataset and vectorizer as above\n",
        "\n",
        "# Train SVM classifier\n",
        "svm_classifier = SVC(kernel='linear', random_state=42)\n",
        "svm_classifier.fit(X_train_counts, y_train)\n",
        "\n",
        "# Predict using SVM\n",
        "y_pred_svm = svm_classifier.predict(X_test_counts)\n",
        "\n",
        "# Predict using Naive Bayes (already trained above)\n",
        "y_pred_nb = nb_classifier.predict(X_test_counts)\n",
        "\n",
        "# Compare accuracies\n",
        "accuracy_svm = accuracy_score(y_test, y_pred_svm)\n",
        "accuracy_nb = accuracy_score(y_test, y_pred_nb)\n",
        "\n",
        "print(f\"SVM Accuracy: {accuracy_svm:.4f}\")\n",
        "print(f\"Naive Bayes Accuracy: {accuracy_nb:.4f}\")\n"
      ],
      "metadata": {
        "id": "-sD4Xh5VNywJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q35. Write a Python program to perform feature selection before training a Naïve Bayes classifier and compare results\n",
        "from sklearn.feature_selection import SelectKBest, chi2\n",
        "\n",
        "# Feature selection - select top 5 features (adjust k as needed)\n",
        "selector = SelectKBest(chi2, k=5)\n",
        "X_train_selected = selector.fit_transform(X_train_counts, y_train)\n",
        "X_test_selected = selector.transform(X_test_counts)\n",
        "\n",
        "# Train Naive Bayes with selected features\n",
        "nb_classifier_fs = MultinomialNB()\n",
        "nb_classifier_fs.fit(X_train_selected, y_train)\n",
        "\n",
        "# Predict and evaluate\n",
        "y_pred_fs = nb_classifier_fs.predict(X_test_selected)\n",
        "\n",
        "print(\"Accuracy without feature selection:\", accuracy_score(y_test, y_pred))\n",
        "print(\"Accuracy with feature selection:\", accuracy_score(y_test, y_pred_fs))\n",
        "print(\"\\nClassification Report with feature selection:\\n\", classification_report(y_test, y_pred_fs))\n"
      ],
      "metadata": {
        "id": "Lw1uYRMsOcKO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q36. Write a Python program to train an SVM Classifier using One-vs-Rest (OvR) and One-vs-One (OvO) strategies on the Wine dataset and compare their accuracy\n",
        "from sklearn import datasets\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.multiclass import OneVsRestClassifier, OneVsOneClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load Wine dataset\n",
        "wine = datasets.load_wine()\n",
        "X, y = wine.data, wine.target\n",
        "\n",
        "# Split dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# OvR SVM\n",
        "ovr_clf = OneVsRestClassifier(SVC(kernel='linear', random_state=42))\n",
        "ovr_clf.fit(X_train, y_train)\n",
        "y_pred_ovr = ovr_clf.predict(X_test)\n",
        "accuracy_ovr = accuracy_score(y_test, y_pred_ovr)\n",
        "\n",
        "# OvO SVM\n",
        "ovo_clf = OneVsOneClassifier(SVC(kernel='linear', random_state=42))\n",
        "ovo_clf.fit(X_train, y_train)\n",
        "y_pred_ovo = ovo_clf.predict(X_test)\n",
        "accuracy_ovo = accuracy_score(y_test, y_pred_ovo)\n",
        "\n",
        "print(f\"One-vs-Rest Accuracy: {accuracy_ovr:.4f}\")\n",
        "print(f\"One-vs-One Accuracy: {accuracy_ovo:.4f}\")\n"
      ],
      "metadata": {
        "id": "0cDb7L7rPU1e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q37. Write a Python program to train an SVM Classifier using Linear, Polynomial, and RBF kernels on the Breast Cancer dataset and compare their accuracy\n",
        "from sklearn import datasets\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load Breast Cancer dataset\n",
        "cancer = datasets.load_breast_cancer()\n",
        "X, y = cancer.data, cancer.target\n",
        "\n",
        "# Split dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "kernels = ['linear', 'poly', 'rbf']\n",
        "\n",
        "for kernel in kernels:\n",
        "    clf = SVC(kernel=kernel, random_state=42)\n",
        "    clf.fit(X_train, y_train)\n",
        "    y_pred = clf.predict(X_test)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    print(f\"SVM with {kernel} kernel accuracy: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "id": "xVnfBfxUPbet"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q38. Write a Python program to train an SVM Classifier using Stratified K-Fold Cross-Validation and compute the average accuracy\n",
        "from sklearn import datasets\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import accuracy_score\n",
        "import numpy as np\n",
        "\n",
        "# Load Breast Cancer dataset (or any dataset)\n",
        "cancer = datasets.load_breast_cancer()\n",
        "X, y = cancer.data, cancer.target\n",
        "\n",
        "# Initialize StratifiedKFold\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "accuracies = []\n",
        "\n",
        "for train_index, test_index in skf.split(X, y):\n",
        "    X_train, X_test = X[train_index], X[test_index]\n",
        "    y_train, y_test = y[train_index], y[test_index]\n",
        "\n",
        "    clf = SVC(kernel='rbf', random_state=42)\n",
        "    clf.fit(X_train, y_train)\n",
        "    y_pred = clf.predict(X_test)\n",
        "\n",
        "    acc = accuracy_score(y_test, y_pred)\n",
        "    accuracies.append(acc)\n",
        "\n",
        "print(f\"Average accuracy with Stratified K-Fold CV: {np.mean(accuracies):.4f}\")\n"
      ],
      "metadata": {
        "id": "utiv3u4CPhgx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q39. Write a Python program to train a Naïve Bayes classifier using different prior probabilities and compare performance\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "X, y = load_iris(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Default priors\n",
        "model_default = GaussianNB()\n",
        "model_default.fit(X_train, y_train)\n",
        "y_pred_default = model_default.predict(X_test)\n",
        "acc_default = accuracy_score(y_test, y_pred_default)\n",
        "\n",
        "# Custom priors (example: uniform distribution)\n",
        "custom_priors = [1/3, 1/3, 1/3]\n",
        "model_custom = GaussianNB(priors=custom_priors)\n",
        "model_custom.fit(X_train, y_train)\n",
        "y_pred_custom = model_custom.predict(X_test)\n",
        "acc_custom = accuracy_score(y_test, y_pred_custom)\n",
        "\n",
        "print(f\"Accuracy with default priors: {acc_default:.4f}\")\n",
        "print(f\"Accuracy with custom priors: {acc_custom:.4f}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "O8OnM5wzRE8c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q40. Write a Python program to perform Recursive Feature Elimination (RFE) before training an SVM Classifier and compare accuracy\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.feature_selection import RFE\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "X, y = load_breast_cancer(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Base SVM without RFE\n",
        "model_base = SVC(kernel='linear')\n",
        "model_base.fit(X_train, y_train)\n",
        "acc_base = accuracy_score(y_test, model_base.predict(X_test))\n",
        "\n",
        "# SVM with RFE (select top 10 features)\n",
        "rfe = RFE(estimator=SVC(kernel='linear'), n_features_to_select=10)\n",
        "rfe.fit(X_train, y_train)\n",
        "model_rfe = SVC(kernel='linear')\n",
        "model_rfe.fit(X_train[:, rfe.support_], y_train)\n",
        "acc_rfe = accuracy_score(y_test, model_rfe.predict(X_test[:, rfe.support_]))\n",
        "\n",
        "print(f\"Accuracy without RFE: {acc_base:.4f}\")\n",
        "print(f\"Accuracy with RFE (10 features): {acc_rfe:.4f}\")\n"
      ],
      "metadata": {
        "id": "TcKP0hS7Rdng"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q41. Write a Python program to train an SVM Classifier and evaluate its performance using Precision, Recall, and F1-Score instead of accuracy\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Load dataset\n",
        "X, y = load_wine(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train SVM\n",
        "model = SVC(kernel='rbf')\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate\n",
        "report = classification_report(y_test, y_pred)\n",
        "print(\"Classification Report:\\n\")\n",
        "print(report)\n"
      ],
      "metadata": {
        "id": "_z-llbeiRkXZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q42. Write a Python program to train a Naïve Bayes Classifier and evaluate its performance using Log Loss (Cross-Entropy Loss)\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import log_loss\n",
        "\n",
        "# Load dataset\n",
        "X, y = load_iris(return_X_y=True)\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train Naïve Bayes\n",
        "model = GaussianNB()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict probabilities\n",
        "y_proba = model.predict_proba(X_test)\n",
        "\n",
        "# Calculate log loss\n",
        "loss = log_loss(y_test, y_proba)\n",
        "print(\"Log Loss (Cross-Entropy):\", loss)\n"
      ],
      "metadata": {
        "id": "BakGPQLSRqwC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q43. Write a Python program to train an SVM Classifier and visualize the Confusion Matrix using seaborn\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "\n",
        "# Load dataset\n",
        "X, y = load_wine(return_X_y=True)\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train SVM\n",
        "model = SVC(kernel='rbf')\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Plot using seaborn\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=model.classes_, yticklabels=model.classes_)\n",
        "plt.title('Confusion Matrix')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "s0girmNGRv8Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q44. Write a Python program to train an SVM Regressor (SVR) and evaluate its performance using Mean Absolute Error (MAE) instead of MSE\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "\n",
        "# Load dataset\n",
        "data = fetch_california_housing()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train SVR\n",
        "model = SVR(kernel='rbf')\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate using MAE\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "print(\"Mean Absolute Error (MAE):\", mae)\n"
      ],
      "metadata": {
        "id": "gwtVl9V2R0AS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q45. Write a Python program to train a Naïve Bayes classifier and evaluate its performance using the ROC-AUC score\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split into train and test\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train Naïve Bayes model\n",
        "nb_model = GaussianNB()\n",
        "nb_model.fit(X_train, y_train)\n",
        "\n",
        "# Predict probabilities\n",
        "y_probs = nb_model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Evaluate using ROC-AUC score\n",
        "roc_auc = roc_auc_score(y_test, y_probs)\n",
        "print(f\"ROC-AUC Score: {roc_auc:.4f}\")\n"
      ],
      "metadata": {
        "id": "VXNRRAsUR7QO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q46. Write a Python program to train an SVM Classifier and visualize the Precision-Recall Curve\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import precision_recall_curve, average_precision_score\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train SVM with probability estimates enabled\n",
        "svm_model = SVC(probability=True, kernel='rbf')\n",
        "svm_model.fit(X_train, y_train)\n",
        "\n",
        "# Predict probabilities\n",
        "y_scores = svm_model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Compute Precision-Recall curve\n",
        "precision, recall, _ = precision_recall_curve(y_test, y_scores)\n",
        "avg_precision = average_precision_score(y_test, y_scores)\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot(recall, precision, label=f'AP = {avg_precision:.2f}')\n",
        "plt.xlabel(\"Recall\")\n",
        "plt.ylabel(\"Precision\")\n",
        "plt.title(\"Precision-Recall Curve (SVM)\")\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "Ha2V-MeXSAtr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}